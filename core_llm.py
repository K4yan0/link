import sys
from llama_cpp import Llama

# Chemin EXACT vers le mod√®le (Attention aux majuscules !)
MODEL_PATH = "models/Qwen2.5-7B-Instruct-Q4_K_M.gguf"

print("üß† Chargement du cerveau en cours... (Cela peut prendre 10-20 secondes)")

try:
    # Initialisation du mod√®le
    llm = Llama(
        model_path=MODEL_PATH,
        n_ctx=4096,       # M√©moire de la conversation
        n_gpu_layers=0,   # 0 pour CPU. Si tu as un GPU Nvidia, mets -1 pour aller plus vite.
        verbose=False     # Pour cacher le blabla technique
    )
except Exception as e:
    print(f"‚ùå Erreur au chargement du mod√®le : {e}")
    print("V√©rifie que le fichier est bien dans le dossier 'models' !")
    sys.exit(1)

# LE COEUR DU PROJET : La consigne p√©dagogique
SYSTEM_PROMPT = """
R√îLE: Tu es un Tuteur de Langues expert et patient. Tes langues : Fran√ßais, Anglais, Japonais, Cor√©en.

R√àGLES DE COMPORTEMENT:
1. NIVEAU DE POLITESSE : 
   - Japonais : Utilise la forme polie (Desu/Masu) UNIQUEMENT. Pas de langage familier, pas de Keigo complexe.
   - Cor√©en : Utilise la forme polie (Haeyo-che / terminaison en -yo).
2. CORRECTION (M√©thode Sandwich) :
   - Si l'user fait une faute, ne dis pas juste "C'est faux".
   - Dis : "Je comprends ce que tu veux dire" -> "Voici la petite erreur" -> "R√©p√®te apr√®s moi : [Phrase Corrig√©e]".
3. CONVERSATION :
   - Pose toujours une question √† la fin pour relancer la discussion.
   - Si l'user te parle en Fran√ßais, r√©ponds en Fran√ßais (et enseigne la langue cible s'il y en a une, sinon converse).
"""

def chat_loop():
    # Historique de la conversation
    history = [
        {"role": "system", "content": SYSTEM_PROMPT}
    ]

    print("\n‚úÖ PROFESSEUR PR√äT ! (Tape 'exit' pour quitter)")
    print("------------------------------------------------")

    while True:
        try:
            user_input = input("\nToi : ")
            if user_input.lower() in ["exit", "quit"]:
                print("√Ä bient√¥t !")
                break

            # Ajout du message user
            history.append({"role": "user", "content": user_input})

            # G√©n√©ration de la r√©ponse
            print("Prof : (r√©fl√©chit...)", end="\r")

            output = llm.create_chat_completion(
                messages=history,
                temperature=0.7, # Cr√©ativit√©
                max_tokens=300   # Longueur max de r√©ponse
            )

            response_text = output['choices'][0]['message']['content']

            # Affichage propre (on √©crase le "r√©fl√©chit...")
            print(f"Prof : {response_text}" + " " * 20)

            # Sauvegarde dans la m√©moire
            history.append({"role": "assistant", "content": response_text})

        except KeyboardInterrupt:
            print("\nArr√™t forc√©.")
            break

if __name__ == "__main__":
    chat_loop()